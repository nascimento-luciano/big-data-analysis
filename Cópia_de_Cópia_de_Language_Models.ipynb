{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9iXrHfa1N8cW",
        "mqkqjRrbuCXq",
        "HagEZ4USCJCM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nascimento-luciano/big-data-analysis/blob/main/C%C3%B3pia_de_C%C3%B3pia_de_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ3gvxUFFaJB",
        "outputId": "e9431aed-19cb-4bd0-fd22-ffa15f38bfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-p1q1h9zd\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-p1q1h9zd\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0.dev0) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.6 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0.dev0) (5.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0.dev0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0.dev0) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163 kB 71.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.24.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.24.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0.dev0) (2022.9.24)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.24.0.dev0-py3-none-any.whl size=5256819 sha256=86d387aa152b186620f683c1021849343216af176619cbc5127089bfeb90fdd5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dutr6ong/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.0 tokenizers-0.13.1 transformers-4.24.0.dev0\n",
            "tokenizers                    0.13.1\n",
            "transformers                  4.24.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aplicando o BERT em uma Tarefa"
      ],
      "metadata": {
        "id": "9iXrHfa1N8cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "example = \"Oswaldo Goeldi nasceu no Brasil foi considerado o maior xilÃ³grafo moderno.\"\n",
        "ner_results = nlp(example)"
      ],
      "metadata": {
        "id": "gXmKMHeChkxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40de2756-4c2a-4bd1-d12b-18b70d124c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Davlan--bert-base-multilingual-cased-ner-hrl/snapshots/6f69c39cadcdba0ab1401fb1f164964e7557e471/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-DATE\",\n",
            "    \"2\": \"I-DATE\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-DATE\": 1,\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-DATE\": 2,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--Davlan--bert-base-multilingual-cased-ner-hrl/snapshots/6f69c39cadcdba0ab1401fb1f164964e7557e471/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Davlan--bert-base-multilingual-cased-ner-hrl/snapshots/6f69c39cadcdba0ab1401fb1f164964e7557e471/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Davlan--bert-base-multilingual-cased-ner-hrl/snapshots/6f69c39cadcdba0ab1401fb1f164964e7557e471/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Davlan--bert-base-multilingual-cased-ner-hrl/snapshots/6f69c39cadcdba0ab1401fb1f164964e7557e471/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-DATE\",\n",
            "    \"2\": \"I-DATE\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-DATE\": 1,\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-DATE\": 2,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Davlan--bert-base-multilingual-cased-ner-hrl/snapshots/6f69c39cadcdba0ab1401fb1f164964e7557e471/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-DATE\",\n",
            "    \"2\": \"I-DATE\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-DATE\": 1,\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-DATE\": 2,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Davlan--bert-base-multilingual-cased-ner-hrl/snapshots/6f69c39cadcdba0ab1401fb1f164964e7557e471/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-DATE\",\n",
            "    \"2\": \"I-DATE\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-DATE\": 1,\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-DATE\": 2,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Davlan--bert-base-multilingual-cased-ner-hrl/snapshots/6f69c39cadcdba0ab1401fb1f164964e7557e471/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "All the weights of BertForTokenClassification were initialized from the model checkpoint at Davlan/bert-base-multilingual-cased-ner-hrl.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for element in ner_results:\n",
        "  print(element['word'], element['entity'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg56QUFChhQA",
        "outputId": "9e7ee5d4-78f5-465e-ea40-95acbcb546d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oswald B-PER\n",
            "##o I-PER\n",
            "Go I-PER\n",
            "##eld I-PER\n",
            "##i I-PER\n",
            "Brasil B-LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinando um Modelo de Linguagem RoBERTa com o Hugging Face"
      ],
      "metadata": {
        "id": "Za3pi-NuMpaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baixando e PrÃ©-processando o corpus\n",
        "\n",
        "Baseado em:\n",
        "\n",
        "*   https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67\n",
        "\n",
        "* https://github.com/attardi/wikiextractor/"
      ],
      "metadata": {
        "id": "mqkqjRrbuCXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baixando o Ãºltimo dump da ptwiki**"
      ],
      "metadata": {
        "id": "y4Q-_9lHtbT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://dumps.wikimedia.org/ptwiki/latest/ptwiki-latest-pages-articles.xml.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk_81Nm9h6VR",
        "outputId": "70c298e5-aabe-4757-ab52-a96095fc376d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-10 16:23:49--  https://dumps.wikimedia.org/ptwiki/latest/ptwiki-latest-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.142, 2620:0:861:2:208:80:154:142\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2020401966 (1.9G) [application/octet-stream]\n",
            "Saving to: â€˜ptwiki-latest-pages-articles.xml.bz2â€™\n",
            "\n",
            "ptwiki-latest-pages 100%[===================>]   1.88G  4.78MB/s    in 6m 50s  \n",
            "\n",
            "2022-10-10 16:30:40 (4.69 MB/s) - â€˜ptwiki-latest-pages-articles.xml.bz2â€™ saved [2020401966/2020401966]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instalar o extrator**"
      ],
      "metadata": {
        "id": "IiPCz4RetjYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install wikiextractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdL9pw04lwaF",
        "outputId": "4b0561a0-be3c-44d1-94f2-8b62e117cb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikiextractor\n",
            "  Downloading wikiextractor-3.0.6-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46 kB 2.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: wikiextractor\n",
            "Successfully installed wikiextractor-3.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baixando e executando o script de ExtraÃ§Ã£o e Limpeza do dump**"
      ],
      "metadata": {
        "id": "95XP4wikt1rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://gist.githubusercontent.com/sgraaf/7c061824b1c57c292faa0a123d95a714/raw/d635bc4c9e0094dd8103df87cf29a452d4491b07/extract_and_clean_wiki_dump.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nywRSly5mUO5",
        "outputId": "5c4360a4-62be-496c-91e5-b4e6f0c8b536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-10 16:30:52--  https://gist.githubusercontent.com/sgraaf/7c061824b1c57c292faa0a123d95a714/raw/d635bc4c9e0094dd8103df87cf29a452d4491b07/extract_and_clean_wiki_dump.sh\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 557 [text/plain]\n",
            "Saving to: â€˜extract_and_clean_wiki_dump.shâ€™\n",
            "\n",
            "\r          extract_a   0%[                    ]       0  --.-KB/s               \rextract_and_clean_w 100%[===================>]     557  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-10 16:30:52 (20.6 MB/s) - â€˜extract_and_clean_wiki_dump.shâ€™ saved [557/557]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############\n",
        "# ~ 40 min #\n",
        "############\n",
        "\n",
        "#!sh /content/extract_and_clean_wiki_dump.sh ptwiki-latest-pages-articles.xml.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdYUych-vHKL",
        "outputId": "5394d645-5c61-4e44-9f5a-b0d2adbce99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wikiextractor'...\n",
            "remote: Enumerating objects: 766, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 766 (delta 13), reused 19 (delta 10), pack-reused 741\u001b[K\n",
            "Receiving objects: 100% (766/766), 1.31 MiB | 827.00 KiB/s, done.\n",
            "Resolving deltas: 100% (446/446), done.\n",
            "Extracting and cleaning ptwiki-latest-pages-articles.xml.bz2 to ptwiki-latest-pages-articles.txt...\n",
            "Succesfully extracted and cleaned ptwiki-latest-pages-articles.xml.bz2 to ptwiki-latest-pages-articles.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download da ptwiki processada"
      ],
      "metadata": {
        "id": "L5PvaEKHGT8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "###   FULL  ####\n",
        "################\n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BQq-3E70L7xRWOwf9nzzjHScILfaPKN1' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1BQq-3E70L7xRWOwf9nzzjHScILfaPKN1\" -O ptwiki-latest-pages-articles.txt && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJWo17emGUGS",
        "outputId": "c7b79123-f902-4071-970d-bdc6e28f798e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-11 00:10:30--  https://docs.google.com/uc?export=download&confirm=t&id=1BQq-3E70L7xRWOwf9nzzjHScILfaPKN1\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.202.139, 173.194.202.138, 173.194.202.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.202.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0g-4k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vu0i1icbcpi6dvppbnjv6foboa8aokf0/1665447000000/17830184011945745179/*/1BQq-3E70L7xRWOwf9nzzjHScILfaPKN1?e=download&uuid=862015d8-0c5f-4760-8c33-ece7bfb26ee4 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-10-11 00:10:30--  https://doc-0g-4k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vu0i1icbcpi6dvppbnjv6foboa8aokf0/1665447000000/17830184011945745179/*/1BQq-3E70L7xRWOwf9nzzjHScILfaPKN1?e=download&uuid=862015d8-0c5f-4760-8c33-ece7bfb26ee4\n",
            "Resolving doc-0g-4k-docs.googleusercontent.com (doc-0g-4k-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0g-4k-docs.googleusercontent.com (doc-0g-4k-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1953163391 (1.8G) [text/plain]\n",
            "Saving to: â€˜ptwiki-latest-pages-articles.txtâ€™\n",
            "\n",
            "ptwiki-latest-pages 100%[===================>]   1.82G   119MB/s    in 17s     \n",
            "\n",
            "2022-10-11 00:10:47 (112 MB/s) - â€˜ptwiki-latest-pages-articles.txtâ€™ saved [1953163391/1953163391]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -100000 /content/ptwiki-latest-pages-articles.txt > train.txt"
      ],
      "metadata": {
        "id": "fp5wONuYvMma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o Tokenizador"
      ],
      "metadata": {
        "id": "qb2c0DIoCfiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/WikiBERT"
      ],
      "metadata": {
        "id": "I6FLUkb9GBjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############\n",
        "# ~ 32 min #\n",
        "############\n",
        "\n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "#paths = [str(x) for x in Path(\"/content/\").glob(\"**/*.txt\")]\n",
        "paths = ['/content/ptwiki-latest-pages-articles.txt']\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "# Save files to disk\n",
        "tokenizer.save_model(\"WikiBERT\")"
      ],
      "metadata": {
        "id": "RvrcqEIKCGuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f728a0-54c7-4232-d6f4-4425d32f8fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['WikiBERT/vocab.json', 'WikiBERT/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download do Tokenizador Treinado**"
      ],
      "metadata": {
        "id": "Ej3penNWolXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1M5vG9HIEUHd8N6A94gTFbQpSoihiP8v4' -O merges.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgAvt7OSokez",
        "outputId": "82396b4e-c33c-41ef-e787-ca09e65fdad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-11 00:11:19--  https://docs.google.com/uc?export=download&id=1M5vG9HIEUHd8N6A94gTFbQpSoihiP8v4\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.117.101, 172.253.117.102, 172.253.117.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.117.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-4k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/kq6uhl1ulsrnkqm405v50936i2aav3r6/1665447075000/17830184011945745179/*/1M5vG9HIEUHd8N6A94gTFbQpSoihiP8v4?e=download&uuid=0325c160-e54b-4b90-8a14-12d3e4909718 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-10-11 00:11:20--  https://doc-04-4k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/kq6uhl1ulsrnkqm405v50936i2aav3r6/1665447075000/17830184011945745179/*/1M5vG9HIEUHd8N6A94gTFbQpSoihiP8v4?e=download&uuid=0325c160-e54b-4b90-8a14-12d3e4909718\n",
            "Resolving doc-04-4k-docs.googleusercontent.com (doc-04-4k-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-04-4k-docs.googleusercontent.com (doc-04-4k-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 517148 (505K) [text/plain]\n",
            "Saving to: â€˜merges.txtâ€™\n",
            "\n",
            "merges.txt          100%[===================>] 505.03K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-10-11 00:11:20 (116 MB/s) - â€˜merges.txtâ€™ saved [517148/517148]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1v1an01n8PLDozo3zWOA57FB_AbO6g6i8' -O vocab.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpzqv0sHp_b5",
        "outputId": "039e68e8-f066-42e0-fb34-cd775b2c7f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-11 00:11:23--  https://docs.google.com/uc?export=download&id=1v1an01n8PLDozo3zWOA57FB_AbO6g6i8\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.117.101, 172.253.117.102, 172.253.117.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.117.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-4k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vmvdsbn4e7g7532vqd6adi64fihhrli8/1665447075000/17830184011945745179/*/1v1an01n8PLDozo3zWOA57FB_AbO6g6i8?e=download&uuid=adb82b4f-d457-45e5-9319-ba907d4cc088 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-10-11 00:11:23--  https://doc-08-4k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vmvdsbn4e7g7532vqd6adi64fihhrli8/1665447075000/17830184011945745179/*/1v1an01n8PLDozo3zWOA57FB_AbO6g6i8?e=download&uuid=adb82b4f-d457-45e5-9319-ba907d4cc088\n",
            "Resolving doc-08-4k-docs.googleusercontent.com (doc-08-4k-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-08-4k-docs.googleusercontent.com (doc-08-4k-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 871051 (851K) [application/json]\n",
            "Saving to: â€˜vocab.jsonâ€™\n",
            "\n",
            "vocab.json          100%[===================>] 850.64K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2022-10-11 00:11:24 (103 MB/s) - â€˜vocab.jsonâ€™ saved [871051/871051]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Movendo os arquivos de tokenizaÃ§Ã£o**"
      ],
      "metadata": {
        "id": "dDmuB3wwqfrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/merges.txt /content/WikiBERT/\n",
        "!mv /content/vocab.json /content/WikiBERT/"
      ],
      "metadata": {
        "id": "kT5yV5AYqRxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testando o Tokenizador**"
      ],
      "metadata": {
        "id": "7lFnyra_w9vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"/content/WikiBERT/vocab.json\",\n",
        "    \"/content/WikiBERT/merges.txt\",\n",
        ")"
      ],
      "metadata": {
        "id": "dIKp8_R0DYsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "metadata": {
        "id": "SOCc6AGVxZtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"Meu nome Ã© Joaquim!\").tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeqFXKjGxeqw",
        "outputId": "74736277-d6b4-4f08-81d8-388447700567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'Meu', 'Ä nome', 'Ä ÃƒÂ©', 'Ä Joaquim', '!', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o Modelo"
      ],
      "metadata": {
        "id": "yYBNI8r9zl9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9lKZuvvxm1W",
        "outputId": "98a3aa91-0199-4830-e41d-5684c4ac7a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "metadata": {
        "id": "UIKmAd02zysG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5euukrhD1OAF",
        "outputId": "79ca76de-f125-474a-9612-de6f0b1f0aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.23.0.dev0\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 52000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"WikiBERT\", max_len=512)"
      ],
      "metadata": {
        "id": "9sNWSroJ1Z5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Passando as configuraÃ§Ãµes para o Modelo**"
      ],
      "metadata": {
        "id": "Y_ZOLll78vYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)"
      ],
      "metadata": {
        "id": "6_XHCKvS6qgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizando o arquivo de treino**"
      ],
      "metadata": {
        "id": "1v9YnN2u8-7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"train.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlq_hJf863kC",
        "outputId": "98ad138d-4781-48c5-cd33-663db8caab63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "metadata": {
        "id": "ZxlEKXLi9FBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finalmente, treinar o modelo!**"
      ],
      "metadata": {
        "id": "rOgemG439cew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/WikiBERT\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI8DDMSk9bq6",
        "outputId": "877c08bd-6b7c-4709-f128-ecf888e739c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "c5Ua1DuV-aph",
        "outputId": "b87ff415-4c86-4786-da5a-e43ed33ed489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 100000\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1563\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1563/1563 30:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.835400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>7.338200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>7.227100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 30min 11s, sys: 5.72 s, total: 30min 17s\n",
            "Wall time: 30min 17s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1563, training_loss=7.456668397362844, metrics={'train_runtime': 1817.6151, 'train_samples_per_second': 55.017, 'train_steps_per_second': 0.86, 'total_flos': 3315678412800000.0, 'train_loss': 7.456668397362844, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Salvando o Modelo**"
      ],
      "metadata": {
        "id": "8t_EgOTS-mWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/Tutorial/WikiBERT/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5z7rX-j-pMw",
        "outputId": "f9ee82f5-69cd-4dea-e558-e0b1013db40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Tutorial/WikiBERT/\n",
            "Configuration saved in /content/drive/MyDrive/Tutorial/WikiBERT/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Tutorial/WikiBERT/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando o Modelo Treinado"
      ],
      "metadata": {
        "id": "T0t2kEJQ-30H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1--9m2b0srAJl3gbkAlbiOmh_dAdX8PUW' -O config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6NKLqBqjjGF",
        "outputId": "a9b3446a-f005-4859-ca39-25fa64628020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-11 01:01:53--  https://docs.google.com/uc?export=download&id=1--9m2b0srAJl3gbkAlbiOmh_dAdX8PUW\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.203.139, 173.194.203.101, 173.194.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.203.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0c-88-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6grucj0g2goaghu82hbhkefioo1759o5/1665450075000/17537572945470902047/*/1--9m2b0srAJl3gbkAlbiOmh_dAdX8PUW?e=download&uuid=c3e33cfd-d982-4ecc-9031-cedada93fe5f [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-10-11 01:01:54--  https://doc-0c-88-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6grucj0g2goaghu82hbhkefioo1759o5/1665450075000/17537572945470902047/*/1--9m2b0srAJl3gbkAlbiOmh_dAdX8PUW?e=download&uuid=c3e33cfd-d982-4ecc-9031-cedada93fe5f\n",
            "Resolving doc-0c-88-docs.googleusercontent.com (doc-0c-88-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0c-88-docs.googleusercontent.com (doc-0c-88-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 641 [application/json]\n",
            "Saving to: â€˜config.jsonâ€™\n",
            "\n",
            "config.json         100%[===================>]     641  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-11 01:01:54 (26.3 MB/s) - â€˜config.jsonâ€™ saved [641/641]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-09h1HstcCKS30-77kxkzQFZY-SUN4gH' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-09h1HstcCKS30-77kxkzQFZY-SUN4gH\" -O pytorch_model.bin && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFNnR7iSkC6H",
        "outputId": "becca393-9f5a-4716-f02f-d64b0d7834ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-11 01:03:48--  https://docs.google.com/uc?export=download&confirm=t&id=1-09h1HstcCKS30-77kxkzQFZY-SUN4gH\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.117.100, 172.253.117.102, 172.253.117.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.117.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0o-88-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jqormllfiirbjdnsjjfvjuoo6nklkirp/1665450225000/17537572945470902047/*/1-09h1HstcCKS30-77kxkzQFZY-SUN4gH?e=download&uuid=0fe41757-05f0-40f5-bffe-f77ee2e011dd [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-10-11 01:03:48--  https://doc-0o-88-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jqormllfiirbjdnsjjfvjuoo6nklkirp/1665450225000/17537572945470902047/*/1-09h1HstcCKS30-77kxkzQFZY-SUN4gH?e=download&uuid=0fe41757-05f0-40f5-bffe-f77ee2e011dd\n",
            "Resolving doc-0o-88-docs.googleusercontent.com (doc-0o-88-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0o-88-docs.googleusercontent.com (doc-0o-88-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 334056939 (319M) [application/x-zip]\n",
            "Saving to: â€˜pytorch_model.binâ€™\n",
            "\n",
            "pytorch_model.bin   100%[===================>] 318.58M  78.4MB/s    in 3.7s    \n",
            "\n",
            "2022-10-11 01:03:52 (85.5 MB/s) - â€˜pytorch_model.binâ€™ saved [334056939/334056939]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-6ycnq4LOBwnGDkCl5bT32KpaJ-haUnM' -O training_args.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93LvAz_nkeVQ",
        "outputId": "6cf43aeb-4542-4f83-c57d-e04e0c56aa22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-11 01:05:24--  https://docs.google.com/uc?export=download&id=1-6ycnq4LOBwnGDkCl5bT32KpaJ-haUnM\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.202.113, 173.194.202.138, 173.194.202.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.202.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-88-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9ikn6c217af7khfdbbffrnscnmfdcj04/1665450300000/17537572945470902047/*/1-6ycnq4LOBwnGDkCl5bT32KpaJ-haUnM?e=download&uuid=0dcdbabc-5252-4d2d-b6a1-f660b63c02c0 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-10-11 01:05:24--  https://doc-0s-88-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9ikn6c217af7khfdbbffrnscnmfdcj04/1665450300000/17537572945470902047/*/1-6ycnq4LOBwnGDkCl5bT32KpaJ-haUnM?e=download&uuid=0dcdbabc-5252-4d2d-b6a1-f660b63c02c0\n",
            "Resolving doc-0s-88-docs.googleusercontent.com (doc-0s-88-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0s-88-docs.googleusercontent.com (doc-0s-88-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3375 (3.3K) [application/x-zip]\n",
            "Saving to: â€˜training_args.binâ€™\n",
            "\n",
            "training_args.bin   100%[===================>]   3.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-11 01:05:24 (56.5 MB/s) - â€˜training_args.binâ€™ saved [3375/3375]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/config.json /content/WikiBERT\n",
        "!mv /content/pytorch_model.bin /content/WikiBERT\n",
        "!mv /content/training_args.bin /content/WikiBERT"
      ],
      "metadata": {
        "id": "USsGC0lJk1KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adivinhando a prÃ³xima palavra**"
      ],
      "metadata": {
        "id": "XVLheCIq-7wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"/content/WikiBERT\",\n",
        "    tokenizer=\"/content/WikiBERT\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJpwC-_J-4BW",
        "outputId": "490735a2-2052-44f6-fa17-28b21d4b107d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/WikiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/WikiBERT\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file /content/WikiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/WikiBERT\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading weights file /content/WikiBERT/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at /content/WikiBERT.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file /content/WikiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/WikiBERT\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading file vocab.json\n",
            "loading file merges.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file /content/WikiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/WikiBERT\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file /content/WikiBERT/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/WikiBERT\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"Mark Rothko nasceu em <mask>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5HK_K_7_pxP",
        "outputId": "6d4ff572-5115-4774-a785-8cb7bd896f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5601575970649719,\n",
              "  'token': 18,\n",
              "  'token_str': '.',\n",
              "  'sequence': 'Mark Rothko nasceu em.'},\n",
              " {'score': 0.04723898693919182,\n",
              "  'token': 30,\n",
              "  'token_str': ':',\n",
              "  'sequence': 'Mark Rothko nasceu em:'},\n",
              " {'score': 0.007757669314742088,\n",
              "  'token': 354,\n",
              "  'token_str': ' (',\n",
              "  'sequence': 'Mark Rothko nasceu em ('},\n",
              " {'score': 0.005470750853419304,\n",
              "  'token': 339,\n",
              "  'token_str': ' \"',\n",
              "  'sequence': 'Mark Rothko nasceu em \"'},\n",
              " {'score': 0.0053644790314137936,\n",
              "  'token': 263,\n",
              "  'token_str': ' de',\n",
              "  'sequence': 'Mark Rothko nasceu em de'}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}